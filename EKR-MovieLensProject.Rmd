---
title: "MovieLens Dataset Analysis"
author: "E.K. RIHANI"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    number_sections: true   # Add section numbering
    toc: true
    toc_depth: 3

header-includes:
indent: true   # Indent paragraphs
bibliography: packages.bib #, references.bib
references:
  - id: movielens
    title: "The MovieLens Datasets: History and Context"
    author: F. Maxwell Harper and Joseph A. Konstan
    url: http://dx.doi.org/10.1145/2827872
    DOI: 10.1145/2827872
    publisher: ACM
    type: article-journal
    issued:
      year: 2016

csl: https://www.zotero.org/styles/vancouver-superscript
#- \usepackage{indentfirst} # Indent first line of each paragraph
---

```{r setup, include=FALSE}
load("EKR-MovieLens.RData")
if(!require(ggpubr)) install.packages("ggpubr", repos = "http://cran.us.r-project.org")
if(!require(rmarkdown)) install.packages("rmarkdown", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
library(ggpubr)   # Combine plots (ggarrange)
library(rmarkdown)
library(knitr)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
knitr::write_bib(c("tidyverse", "caret", "data.table", "reshape2", "ggplot2", "ggrepel", "recommenderlab", "stringr", "ggpubr", "rmarkdown", "knitr"), "packages.bib")
```
\newpage
\section{Overview}
\subsection{Goal of the study}
\paragraph*{}
Recommendation systems are algorithms that try to predict users' preferences in order to improve the suggestions one company can make to users. These algorithms are widely used in movies platforms, web stores and search engines in order to improve sales, but also the user experience and the accuracy of the suggested products, movies or links.
\paragraph*{}
The aim of this study is to analyze a MovieLens dataset[@movielens] and try to create a model that can predict users' future preferences based on the ratings they gave on the movies they previously saw, using data analysis and machine learning techniques.
\paragraph*{}
The main goal is to predict users' probable future ratings with the lowest possible error. A secondary goal is to obtain these results in a reasonable amount of time, on a desktop computer.

\subsection{The dataset}
\paragraph*{}
The MovieLens datasets were created by the GroupLens research lab, from the Department of Computer Science and Engineering at the University of Minnesota.
\paragraph*{}
The dataset used for this study is the 10M dataset (<https://grouplens.org/datasets/movielens/10m/>). The 10M MovieLens dataset contains `r total_number_ratings` ratings for `r total_number_movies` movies, from `r total_number_users` users, which translates into a `r total_number_movies` $\times$ `r total_number_users` matrix, with `r round(total_number_movies*total_number_users/1e6, 1)` millions cells, `r round((1 - total_number_ratings/(total_number_movies*total_number_users))*100, 1)` % of which are empty. 
\paragraph*{}
In layman's terms, the goal of this project is to fill these empty cells as accurately as possible.
\paragraph*{}
The provided information includes the user's unique ID (`r column_names[1]`), the movie's unique ID (`r column_names[2]`), the rating given by the user (`r column_names[3]`), the timestamp of the rating (`r column_names[4]`), the title of the movie (`r column_names[5]`) and its genre (`r column_names[6]`).
\paragraph*{}
This dataset was split into a training set (*edx*, 90% of the ratings) that will be used for building, training and tuning the recommendation system and a validation set (*validation*, 10% of the ratings) that will exclusively be used for the final evaluation of the accuracy of our recommendation system.

\newpage
\section{Methods and Analysis}
\paragraph*{}
This project will be articulated around the use of the *recommenderlab* package, which is a very powerful library for building recommendation systems that was recommended by Pr. Rafael Irizarry in his data science course.
\paragraph*{}
The configuration will be described, since it can have an impact on results and performance.Then recommenderlab library and its different recommendation systems will be briefly introduced. Their respective performances will be evaluated, both in computing time and accuracy, in order to select the most promising models. Their tuning parameters will then be tuned, and the best model will finally be used against the validation set.
\paragraph*{}
Code extracts will be included to illustrate how to setup and use the *recommenderlab* package.

\subsection{Computer and R configuration}
\paragraph*{}
The code, benchmarks and report were run on the following computer configuration :

* CPU : AMD Ryzen 5 5600G
* RAM : 2x16 GB DDR4-3200
* SSD : Crucial P5 M2 NVMe
* OS : Xubuntu Linux
* R : version 3.6.3
* Packages : tidyverse[@R-tidyverse] (v1.3.1), caret[@R-caret] (v6.0-88), data.table[@R-data.table] (v1.14.0), reshape2[@R-reshape2] (v1.4.4), , ggrepel[@R-ggrepel] (v0.9.1), recommenderlab[@R-recommenderlab] (v0.2-7), ggpubr[@R-ggpubr] (v0.4.0), rmarkdown[@R-rmarkdown] (v2.11), knitr[@R-knitr] (v1.34).

\paragraph*{}
The code was also tested on a Windows 7 x64 SP1 computer with 16 GB RAM and R 4.1.0. It was not tested on any Mac OS system.
\subsection{Preparing data for the recommenderlab package}
\paragraph*{}
The recommenderlab package uses several specific data formats, all based on a single sparse matrix, where the ratings (binary or numeric) are located at the intersection of the user (*userId*) row and the rated object (*movieId*) column. Because ratings are numeric, this study will use the *realRatingMatrix* format.
\paragraph*{}
Since recommenderlab only uses ratings, one notable limitation of this study is that it will not use some potentially useful information such as the moment of the rating or the year or gender of the movie to improve the accuracy of the analysis.
\paragraph*{}
The first step of the data preparation involves the transformation of the *edx* dataset into a $userId \times movieId$ matrix (*edx_rrm*) with *rating* at each user/movie intersection. The acast::reshape2 function can be used to perform this operation. The resulting matrix can then be easily converted in a *realRatingMatrix* format.

```{r, eval = FALSE}
edx_rrm <- acast(edx, userId ~ movieId, value.var = "rating")
edx_rrm <- as(edx_rrm, "realRatingMatrix")
```

The same operation must be performed on the validation data set. To make sure the *validation* isn't used until the final validation, we explicitly chose not to use it *at all* until the final steps.
\paragraph*{}
This method has two limits. Firstly, it uses a considerable amount of memory on large datasets and thus requires to use the *memory.limit* function on Windows systems, or to have enough total memory (RAM+swap) on Linux systems. Otherwise, R may return errors or even crash.
\paragraph*{}
Secondly, some recommenderlab models may encounter problems if the training set and the validation sets don't have the same items (movies). This can be the case if a list-splitting method is used, because some items can be present in the training set, but not in the validation set.
\paragraph*{}
Two equivalent strategies can be used : either add the missing items with *NA* ratings in the validation set or remove the useless items in the training set. Since modifications of the validation set are explicitly forbidden, the only option is to remove useless items in the training set, i.e. exclude from the *edx* set all movies that don't exist in the *validation* set, before building the realRatingMatrix.

\subsection{Cross-validation}
\paragraph*{}
In order to compare the algorithms and tuning parameters, the cross-validation gold standard is probably the hold-out method. This method holds out a portion of the initial dataset only for the final evaluation, and splits the other part into training and validation subsets.
\paragraph*{}
In our case, the *movielens* dataset was already split into an *validation* (final evaluation) and *edx* set that has to be split into a training and validation set. Several strategies can be used to perform this split, such as a basic split-train or a more elaborate K-fold cross validation. 
\paragraph*{}
Since our goal is to be able to compare most *recommenderlab* algorithms and tuning parameters in a reasonable amount of time, the split-train validation is a good choice, because this method is K times faster than a K-fold cross validation although it typically gives noisier and more biased results for algorithm selection and tuning than a K-fold cross-validation method.
\paragraph*{}
However, in order to be able to compare the performance of nine algorithms (some of them being very slow) in a reasonnable time, and perform all the hyperparameter tunings, all initial benchmarks and validations were run on a smaller subset of the *edx* dataset. In other words, we chose to "undertrain" in all model selections and hyperparameter tuning steps. This strategy gives much faster results, but all preliminary performance metrics should of course be considered as rough estimates. The main caveat of this strategy is that the larger amount of bias and noise can lead to less-than-optimal model or hyperparameter choices, and a much larger final error.
\paragraph*{}
The split-train method can be performed by splitting the *edx* (training/validation) dataset with a regular index sampling method and then building the corresponding realRatingMatrix, or more simply splitting the training realRatingMatrix (*edx_rrm*) into smaller matrices. For example, for a 90/10 training/prevalidation ratio :

```{r, eval = FALSE}
test_index <- sample(x = seq(1, nrow(edx_rrm)), size = nrow(edx_rrm)*0.1, replace = FALSE)
edx_rrm_train <- edx_rrm[-test_index]
edx_rrm_test <- edx_rrm[test_index]
```

The *recommenderlab* package conveniently provides an *evaluationScheme* function that can perform this simple split-train method or build K-fold cross validation datasets. This packages also has an *evaluation* function that can be very useful for accuracy evaluation of models and hyperparameters, especially in association with K-fold cross validation. These function can prove to be very useful, notably on smaller datasets. However, the *evaluationScheme* function proved to be way to slow for the 10M movielens dataset on a regular desktop computer.
\paragraph*{}
Apart from the possibility of sub-optimal model selection and tuning, all the choices made in these splitting steps for benchmarking/tuning purposes have absolutely no impact on the final RMSE, since the *edx* and *validation* datasets were already created using a list-based splitting method with a fixed seed.

\subsection{Performance metrics}
\paragraph*{}
Several metrics can be used to evaluate the accuracy of the prediction, such as the Mean Average Error (MAE, consistent with the data units), the Mean Square Error (MSE, penalizes large errors) and the Root Mean Square Error (RMSE, penalizes large errors **and** is consistent with the data units). The metric used in this study to evaluate the accuracy of the prediction is the root-mean-squared error (RMSE), given by : $$RMSE = \sqrt{ \frac{1}{N} \sum_{m,u}\left(\hat{y}_{m,u} -  y_{m,u}\right)^{2}}$$ With N the number of ratings, $\hat{y}_{m,u}$ the rating (for the movie m, by the user u) that is predicted by the model built with the training set and $y_{m,u}$ the actual rating of the testing set. The lower the RMSE, the more accurate the model.
\paragraph*{}
Recommenderlab conveniently provides a *calcPredictionAccuracy* function that computes the MAE, MSE and RMSE. As an example (using a LIBMF method) the recommendation, prediction and accuracy evaluation can be run by :

```{r, eval = FALSE}
recommendation <- Recommender(data = edx_rrm, method = "LIBMF")
prediction <- predict(recommendation, validation_rrm, type = "ratingMatrix")
accuracy <- calcPredictionAccuracy(validation_rrm, prediction)
```

Another very useful and easy to evaluate metric is the time needed to execute the recommendation, prediction and accuracy steps. The code used in our study is quite simple :
```{r, eval = FALSE}
start_time <- Sys.time()
# Insert_code_here #
end_time <- Sys.time()
running_time <- difftime(end_time, start_time, units = "secs")
```
The *units* argument in the *difftime* function ensures all units are kept consistent. Mixing minutes and seconds can be problematic, especially if these results are used for plotting.

\subsection{About the recommenderlab methods}
\paragraph*{}
The *Random* method, as its name suggests, assigns random ratings to all movies for each user. This method can be used to benchmark other models.
\paragraph*{}
The *Popular* method is based on the most rated items, and will thus recommend the most viewed movies. The rating is evaluated using a distance method.
\paragraph*{}
The *UBCF* and *IBCF* methods are respectively an user-based and an item-based collaborating filtering methods. These methods are the oldest recommender methods (1992) and are based on a "similar users like similar things" strategies. UBCF focuses on the users and is based on the hypothesis that users with comparable preferences will rate movies similarly, and thus tries to find for each user a k-neighborhood of the most comparable users (by using cosine similarity, by default) and aggregate their ratings to form a prediction. IBCF works similarly, but is focused on items, the core hypothesis being that users prefer movies that are similar to movies they already like. The IBCF method uses a comparable approach as the UBCF but tries to find similarities between movies by computing the k-neighborhood of movies instead of users.
\paragraph*{}
The *ALS*, *ALS_implicit* and *LIBMF* methods are all based on the same mathematical concept : matrix factorization. Matrix factorization is a widely used method that tries to approximate our entire rating matrix $R_{u \times m}$ as the product of smaller-sized matrices $P_{k \times u}$ and $Q_{k \times m}$ : $R \approx P'Q$. In other words, each user and each item are summarized by *k* dimensional vectors, where *k* is a small fixed number. The underlying mathematical problem is then to minimize the distance (least squares error) between each known $r_{u,m}$ rating and the corresponding intersection of the product of our two smaller-sized matrices $p_{u}^{T}.q_{m}$.
\paragraph*{}
In ALS and ALS_implicit optimization strategies, $p_{u}$ is fixed and $q_{m}$ is optimized by minimizing the square error, then $q_{m}$ is fixed and then $p_{u}$ is optimized, hence the *Alternating Least Square* name. The ALS method is used for explicit data, which are strictly correlated to the values we are trying to predict. The ALS_implicit method is designed for implicit data that reflects the interaction between the user and the movie and is only indirectly linked to the rating. In our application, the ALS method will try to predict user ratings using user ratings, whereas the ALS_implicit method tries to predict user ratings with clicks, fast-forwards, number of times viewed... Since these kind of data isn't present in our dataset, using the ALS_implicit method makes little sense.
\paragraph*{}
*LIMBF* is an open-source library was created by Chin et al, from the Taiwan University, in 2014. This algorithm uses a gradient descent approach, which is an iterative algorithm that compute the local gradient of this distance and adjusts the $p_{u}$ and $q_{m}$ *against* the gradient in order to approach a better value. This specific library also aims to make full use of the computing power of modern processors (SSE and AMX instructions, multithreading) in Matrix Factorization.
\paragraph*{}
The *SVD* and *SVDF* (Funk SVD) methods are both Singular Value Decomposition methods **[ A FINIR !!!!]**

\subsection{Performance of the different models}
\paragraph*{}
Some models may be faster and/or more accurate than others and thus be more interesting for our study. In order to evaluate the model performance, we will use smaller subsets that will allow quicker computation and comparison of the respective performance of all the recommenderlab methods.
\paragraph*{}
The *edx* dataset was split into smaller subsets that will allow a quicker evaluation of our different models. These smaller subsets are then split into a training (90%) and validation (10%) sets.
\paragraph*{}
Training and validation are then performed for each dataset. The computing time of the training, prediction and validation steps is measured. The accuracy of the prediction (RMSE) is also evaluated. We can then create a scatterplot of the RMSE vs compute time of our different models, and plot lines for our RMSE criterias (RMSE > 0.900 and RMSE < 0.865).
\paragraph*{}
```{r echo = FALSE, fig.cap = "Benchmark of the recommenderlab methods with a 0.5% subset size"}
plot(plot_time_rmse1 + theme_bw())
```
```{r echo = FALSE}
kable(benchmark_result1 %>% select("RMSE", "time"),
  align = "cc",
  caption = "Benchmark of the methods on a 0.5% subset")
```

As one can expect, the random model is among the fastest, but performs quite poorly in terms of accuracy.
\paragraph*{}
The Popular, SVD, UBCF and LIBMF are clear winners, being all in the "fast and accurate" quadrant of our RMSE vs time plot. The accuracy of the SVDF method is comparable with the accuracy of SVD, but SVDF seems to be much slower.
\paragraph*{}
The ALS method seems to be quite slow, but accurate. As expected, we can see a large difference between explicit and implicit ALS methods. Our data are explicit : we use (known) user ratings to predict (unknown) user ratings. Force-feeding these explicit data in an method that was designed around implicit data gives results that are both very slow and inaccurate ($RMSE_{ALSimplicit} > RMSE_{random}$). 
\paragraph*{}
As said before, the UBCF method performed quite well. However, the IBCF was so slow it wasn't included in the plot. This can look quite suprising at first. However, the UBCF is based on users, while the IBCF is based on items (movies). In other words, UBCF is row-based, while IBCF is column-based : since we defined our smaller datasets as a smaller matrix row-wise, the UBCF worked on 0.5% of the rows, while the IBCF method had to work on 100% of the columns, hence the poor performance, both on time and accuracy criterias.
\paragraph*{}
The ALS_implicit and IBCF results can look quite surprising at first, but were predictable in the end. This underline the importance of understanding how these methods work, and not just treat them as "black boxes" or "magic bullets".
\paragraph*{}
We can use these preliminary results to exclude the slowest and most inaccurate models (SVDF, ALS, ALS_implicit, random) for our next benchmarks with larger datasets.
\paragraph*{}
```{r, echo = FALSE, fig.cap = "Benchmark of the recommenderlab models with 2% and 10% subset sizes"}
plot(
  ggarrange(
  plot_time_rmse2 + theme_bw(),
  plot_time_rmse3 + ylab("")+ theme_bw()
  )
)
```

These larger datasets seem to confirm the respective places of the SVD, UBCF, LIBMF and Popular methods : SVD and Popular are fast but not very accurate, UBCF is quite accurate but slow. LIBMF seems to be the most interesting model, being both very accurate and fast.
\paragraph*{}
However, one should keep in mind that these results -- both in time and accuracy -- are for very small datasets. We should thus carry on and try to predict the performance of these models on larger datasets.
\paragraph*{}
```{r, echo = FALSE, fig.cap = "Computing time of the 4 best models (linear and quadratic time scales)"}
plot(
  ggarrange(
  plot_time_size1 + theme_bw(),
  plot_time_size2 + ylab("") + theme_bw(),
  common.legend = TRUE, legend = "right"
  )
)
```
\paragraph*{}
The computing time vs dataset size scatterplot seems to show a lower computing time for SVD, Popular and LIBMF models, while the UBCF model appears to be considerably slower. Furthermore, the computing time of this model seems to exhibit a quadratic behavior (right plot), whereas the other methods seem to be linear.
\paragraph*{}
We can try to predict the computing time for a full-sized dataset :
```{r echo = FALSE}
kable(tsv_models,
  align = "cccccc",
  caption = "Modelling of time vs size behavior of the 4 best models",
  digits = c(0,0,3,2,1,3))
```

\paragraph*{}
The UBCF model, while quite accurate, will probably be too slow for this study.
\paragraph*{}
We will thus select the LIBMF, POPULAR and SVD models. Since all the previous results were obtained with default settings, the next step will be to tune them in order to improve their performance.

\subsection{Tuning the models}

\subsubsection{Selecting the best dataset size}
\paragraph*{}
In order to tune our models, we first have to chose a relevant-sized training set.
\paragraph*{}
```{r out.width = "90%", echo = FALSE}
plot(plot_rmse_size + theme_bw())
```
\paragraph*{}
According to the RMSE vs size scatterplot, our 3 selected models seem to show stability for $size \ge 0.20$.\
Two models already have adequate performance out of the box, with $RMSE_{pop} =$ `r benchmark_result9["POPULAR",]$RMSE` in $t_{pop} =$ `r benchmark_result9["POPULAR",]$time` s for the Popular model and $RMSE_{lib} =$ `r benchmark_result9["LIBMF",]$RMSE` in $t_{lib} =$ `r benchmark_result9["LIBMF",]$time` s for LIBMF. Before tuning, SVD seems to lag behind, both in RMSE (`r benchmark_result9["SVD",]$RMSE`) and time (`r benchmark_result9["SVD",]$time` s) performance.

\subsubsection{Tuning the POPULAR model}
\paragraph*{}
The POPULAR method only has one parameter : *normalize*, which defines the normalization method used to counter the user biases. This parameter can be set on *center* or *Z-score*. The *center* normalization uses the mean of the ratings, whereas the *Z-score* normalization goes further by dividing by the standard deviation, which allows this normalization method to better handle outliers.
```{r echo = FALSE}
kable(table_pop_normalize, caption = "Popular model, *normalize* parameter tuning")
```
\paragraph*{}
Quite surprisingly, our results show that the Z-score normalization, albeit more refined, produces slightly less accurate predictions than the center normalization, but is sensibly faster.  
\paragraph*{}
In conclusion, the Popular method, while quite simple to understand and to use, is not accurate enough (RMSE = `r min(table_pop_normalize$rmse)` ) for our study.

\subsubsection{Tuning the SVD model}
\paragraph*{}
The SVD method has three parameters :

* *k* : the rank of the SVD approximation (default : 10)
* *maxiter* : the maximum number of iterations (default : 100)
* *normalize* : the normalization method, *center* or *Z-score* (default : center
\paragraph*{}
```{r out.width = "90%", echo = FALSE}
plot(plot_fitting5 + 
  geom_hline(yintercept = 0.9, linetype = "dotted", color = "darkred", alpha = 0.5) +   # Minimal objective
  geom_hline(yintercept = 0.865, linetype = "dotted", color = "darkgreen", alpha = 0.5) +   # Optimal objective
  theme_bw())
```
\paragraph*{}
Raising the *k* parameter vastly improves the accuracy of the prediction. However, this has a considerable cost in computing time. Getting RMSE below the optimal value (RMSE $\le$ 0.865) requires to set $k >$ 50 (RMSE = `r SVD_optimal_k[1,]$rmse` in `r SVD_optimal_k[1,]$time` s for *k* = `r SVD_optimal_k[1,]$value`).
\paragraph*{}
```{r out.width = "90%", echo = FALSE}
plot(plot_fitting6 + theme_bw())
```
\paragraph*{}
The *maxiter* parameter doesn't seem to have any meaningful impact on this study : all RMSE are identical, and all computing times seem randomly distributed in a `r SVD_maxiter_timespan` seconds time span.
\paragraph*{}
```{r out.width = "90%", echo = FALSE}
kable(table_svd_normalize, caption = "SVD model, *normalize* parameter tuning")
```
\paragraph*{}
The *normalize* parameter does not seem to have a sensible impact on RMSE or computing time.
\paragraph*{}
In conclusion, the SVD model can be used to reach RMSE $\le$ 0.865. Better accuracy (lower RMSE) can be obtained with higher ranks of SVD approximations ($k >$ 50), but this accuracy has a strong cost in computing time. The *maxiter* and *normalize* factors don't seem to have any meaningful impact on accuracy or computing time.  

\newpage
\subsubsection{Tuning the LIBMF model}
\paragraph*{}
The LIBMF method has four parameters :

* *dim* : the number of latent features (default : 10)
* *costp_l2* : the regularization parameter for the user factor (default : 0.01)
* *costq_l2* : the regularization parameter for the item factor (default : 0.01)
* *nthread* : the number of threads (default : 1)

\paragraph*{}
```{r out.width = "90%", echo = FALSE}
plot(plot_fitting1 + theme_bw())
```
\paragraph*{}
The *dim* parameter has a major effect on accuracy and computing time. Higher values (*dim* = `r LIBMF_best_dim$value`) yield a better accuracy, with RMSE = `r LIBMF_best_dim$rmse`, at the cost of a higher computing time (t = `r LIBMF_best_dim$time` s).
\paragraph*{}
Very high or very low *dim* values seem to show diminishing returns in time or accuracy. We can thus design time/RMSE optimization strategies by computing a composite indicator (such as $time \times RMSE ^{n}$, with $n$ a fixed weight factor), then finding its minimum.
\paragraph*{}
We can for example choose to put a 2:1 weight on RMSE vs time (*n* = 2), to reflect the higher priority given to RMSE -- which is the primary goal of this project -- while also taking computing time into account.
```{r out.width = "90%", echo = FALSE}
plot(plot_fitting1b + theme_bw())
```
\paragraph*{}
With this (arbitrary) 2:1 ratio given on our RMSE/time composite factor (*n* = 2), an optimum seems to be reached for a dimension value of $dim \approx$ `r LIBMF_best_dim_composite$value`.
\paragraph*{}
That *dim* value gives RMSE = `r LIBMF_best_dim_composite$rmse` in `r LIBMF_best_dim_composite$time` seconds.
\paragraph*{}
```{r, echo = FALSE, fig.cap = "Fitting of LIBMF method, user (costp_l2) and item (costq_l2) factor regularization parameters"}
plot(ggarrange(
  plot_fitting2 + ggtitle("Fitting: LIBMF method, costp_l2") + theme_bw(),
  plot_fitting3 + ggtitle("Fitting: LIBMF method, costq_l2") + ylab("") + theme_bw()
  )
)
```
The user (*costp_l2*) and item (*costq_l2*) regularization factors seem to have an important effect on the accuracy and a minor effect on computing time.
\paragraph*{}
The accuracy seems to considerably decrease for higher values of *costp_l2* and *costq_l2*. However, for values below 0.01  (default value), the potential accuracy gains seem to be quite marginal.
\paragraph*{}
Apart from some outliers, most of the *costp_l2* and *costq_l2* values seem to have little effect on computing time, with 95% confidence intervals widths of `r round(LIBMF_costp_CI,2)` seconds for *costp_l2* and `r round(LIBMF_costp_CI,2)` seconds for *costq_l2*.

\paragraph*{}
```{r out.width = "90%", echo=FALSE}
plot(plot_fitting4 + theme_bw())
```
\paragraph*{}
The number of threads (*nthread*) seem to have some meaningful impact in computing time. The RMSE varies very slightly from one *ntrehad* setting to another, but the difference is very small ($\Delta_{RMSE} \approx$ `r round(LIBMF_nthread_rmse_span,3)`) This is an interesting result that shows that the LIMBF multithreading works out of the box, without having to install additional libraries or set the parallelization manually.
\paragraph*{}
However, the improvement in computing time can seem disappointing, since the impact is pretty minor, especially for $nthread \geq 4$.
\paragraph*{}
This can probably be explained by the small size of the sample used for our fitting purposes and the resulting short computation time : the gains obtained by multithreading thus cannot overcome the time and memory used to setup the parallelization. This phenomenon is usually known as *parallelism overhead*.
\paragraph*{}
A more significant performance boost with higher values of *nthread* can probably be expected on larger datasets.

\paragraph*{}
In conclusion, LIBMF seems to be, by far, the most interesting model, with both excellent accuracy and computing times, even with default parameters.
\paragraph*{}
The most impactful way to improve accuracy of the LIMBF model seems to increase the number of latent features (*dim*). However, this factor has a strong impact on computing time. Our results are perfectly consistent with those obtained by the creators of the LIBMF library in their initial tests. The excellent performance of the LIBMF model allows the user to choose between good accuracy and excellent computing time, or outstanding accuracy in a reasonable time.
\paragraph*{}
The user factor (*costp_l2*) and item factor (*costq_l2*) regularization parameters seem to have some minor impact, with lower values giving slightly better accuracy, with little effect in computing time. However, since the LIMBF model uses these factors to penalize potential overfitting, it may be preferable to keep the default values.
\paragraph*{}
The number of threads has some impact in computing time, but this impact looks minor, and the expected gains on this small subset are unfortunately counterweighted by the parallelism overhead.

\paragraph*{}
*Section that explains the process and techniques used, including data cleaning, data exploration and visualization, insights gained, and your modeling approach*

\newpage
\section{Results}

\subsection{Evaluation against the \emph{validation} dataset}
\paragraph*{}
Given the previous benchmarks, we chose the following parameters for our final evaluation, against the *validation* dataset :

* Method : LIMBF
* Number of latent features (*dim*) : 400
* Regularization parameter, user factor (*costp_l2*) : 0.01
* Regularization parameter, item factor (*costq_l2*) : 0.01
* Number of threads (*nthread*) : 16

\paragraph*{}
With these settings, we manage to obtain RMSE = `r final_rmse` in `r final_time` s.
\paragraph*{}
As suggested by our previous selection and tuning processes, the LIBMF model provides excellent accuracy in a very reasonable amount of time.

*Section that presents the modeling results and discusses the model performance.*
\newpage
\section{Conclusion}
\paragraph*{}
The aim of this study was to build a recommendation system based on the recommenderlab package and use it to predict user preferences on the Movielens dataset. After splitting the data into a training and validation sets, some data preparation of the training set was performed, notably by converting the sparse dataset into a suitable matrix format. Training and evaluation of nine recommendation algorithms were then performed, using cross-validation on a smaller subset of the original training set. The three best algorithms were then fine-tuned, and the best one (LIBMF) was used against the validation set.
\paragraph*{}
The LIBMF method proved to give outstanding performance, with excellent accuracy (RMSE = `r final_rmse`) in a reasonable amount of time (t = `r final_time` s).
\paragraph*{}
Although this code was run and tested on two different systems (R 4.1.0 on Windows, R 3.6.3 on Linux), it was not possible to test on any Mac OS system.
\paragraph*{}
Despite being oriented toward both accuracy and performance in terms of computing time of the recommendation algorithm, there is still a considerable room for improvement in the performance field of the selection and tuning processes themselves. These steps could probably benefit from some parallelization and further code optimization.
\paragraph*{}
*Section that gives a brief summary of the report, its limitations and future work*
\newpage
\section{References}
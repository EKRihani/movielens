---
title: "MovieLens Dataset Analysis"
author: "E.K. RIHANI"
date: "07/10/2021"
output: pdf_document
---

```{r setup, include=FALSE}
load("EKR-MovieLens.RData")
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(rmarkdown)
```

# 1. Overview

## 1.1 Goal of the study

The aim of this study is to analyze a MovieLens dataset and try to create a model that can predict users' future preferences based on their current preferences, using data analysis and machine learning techniques.

## 1.2 The dataset

The MovieLens datasets were created by the GroupLens research lab, from the Department of Computer Science and Engineering at the University of Minnesota.\
The dataset used for this study is the 10M dataset (<https://grouplens.org/datasets/movielens/10m/>). The 10M MovieLens dataset contains `r total_number_ratings` ratings for `r total_number_movies` movies, from `r total_number_users` users.\
The provided information includes the user unique ID (`r column_names[1]`), the movie unique ID (`r column_names[2]`), the rating given by the user (`r column_names[3]`), the timestamp of the rating (`r column_names[4]`), the title of the movie (`r column_names[5]`) and its genre (`r column_names[6]`).\
This dataset was split into a training set (*edx*, 90% of the ratings) that will be used for building and training the recommendation system and a validation set (*validation*, 10% of the ratings) that will exclusively be used for the final evaluation of the accuracy of our recommendation system.

# 2. Methods and Analysis

This project will be articulated around the use of the *recommenderlab* package, which is a very powerful library for building recommendation systems that was recommended by Pr. Rafael Irizarry in his data science course.\
We'll first briefly introduce the recommenderlab library and its different recommendation systems, then we'll evaluate their respective performances, both in computing time and accuracy, and we'll then optimize the tuning parameters of our best models before using the best model against our validation set.

## 2.1 Preparing data for the recommenderlab package

The recommenderlab package uses several specific data formats, all based on a single sparse (?) matrix, where the ratings (binary or numeric) are located at the intersection of the user row (*userId*) row (?) and the rated object (*movieId*) column. Since our rating are numeric, this study will use the *realRatingMatrix* format.\
Since recommenderlab only uses ratings, one limitation of this study is that it will not use some potentially useful informations such as the moment of the rating or the year or gender of the movie to improve the accuracy of the analysis.\
The first step of the data preparation involves the transformation of our dataset into a matrix

## 2.2 Comparing the performance of the different models

Several metrics can be used to evaluate the accuracy of the prediction, such as the Mean Average Error (MAE, consistent with the data units), the Mean Square Error (MSE, penalizes large errors) and the Root Mean Square Error (RMSE, penalizes large errors **and** is consistent with the data units). The metric used in this study to evaluate the accuracy of the prediction is the root-mean-squared error (RMSE), given by : $$\sqrt{ \frac{1}{N} \sum_{m,u}\left(\hat{y}_{m,u} -  y_{m,u}\right)^{2}}$$ With N the number of ratings, $\hat{y}_{m,u}$ the rating (for the movie m, by the user u) that is predicted by the model built with the training set and $y_{m,u}$ the actual rating of the testing set. The lower the RMSE, the more accurate the model.\
Recommenderlab conveniently provides a *calcPredictionAccuracy* function that computes the MAE, MSE, RMSE.

Section that explains the process and techniques used, including data cleaning, data exploration and visualization, insights gained, and your modeling approach

# 3. Results

## 3.1. Performance of the different models

We can create a scatterplot of the RMSE vs compute time of our different models.

```{r echo=FALSE}
plot(plot_time_rmse1 + theme_bw())
```

As one can expect, the random model is among the fastest, but performs quite poorly in termes of accuracy.\
The Popular, SVD and LIBMF are clear winners, being all in the "fast and accurate" quadrant of this plot. UBCF appears to be a reasonnable option, too. SVDF's accuracy is on par with SVD, but SVDF seems to be much slower.\
The ALS seems to be quite slow, but accurate. More suprisingly, the ALS_implicit model performs very poorly (being both the slowest and most inaccurate model), despite being based on an alternating least squares strategy like the regular ALS model.

```{r echo=FALSE}
#par(mar = c(4, 4, .2, .1))
plot(plot_time_rmse2, pch = 19 + theme_bw())
plot(plot_time_rmse3, pch = 17 + theme_bw())
```

Bigger datasets seem to confirm the respective places of the SVD, UBCF, LIBMF and Popular methods.

```{r echo=FALSE}
#par(mar = c(4, 4, .2, .1))
plot(plot_time_size1, pch = 19 + theme_bw())
plot(plot_time_size2, pch = 17 + theme_bw())
plot(plot_time_size3, pch = 17 + theme_bw())
```

The computing time vs dataset size scatterplot seems to show a lower computing time for SVD, Popular and LIBMF models, while the UBCF model appears to be considerably slower. Furthermore, the computing time of this model seems to exhibit a quadratic behaviour, whereas the others seem to be linear.\
The UBCF model, while quite accurate, will probably be too slow for this study.\
We will thus select the LIBMF, POPULAR AND SVD models and will tune them to improve their performance.

## 3.2. Tuning the models

### 3.2.1. Selecting the best dataset size

In order to tune our models, we first have to chose a relevant-sized training set.

```{r echo=FALSE}
plot(plot_rmse_size + theme_bw())
```

According to the RMSE vs size scatterplot, our 3 selected models seem to show stability for $size \ge 0.20$.\
Two models already have adequate performance out of the box, with $RMSE_{pop} =$ `r benchmark_result9["POPULAR",]$RMSE` in $t_{pop} =$ `r benchmark_result9["POPULAR",]$time` s for the Popular model and $RMSE_{lib} =$ `r benchmark_result9["LIBMF",]$RMSE` in $t_{lib} =$ `r benchmark_result9["LIBMF",]$time` s for LIBMF. Before tuning, SVD seems to lag behind, both in RMSE (`r benchmark_result9["SVD",]$RMSE`) and time (`r benchmark_result9["SVD",]$time` s) performance.

<!-- PARALLELISATION ? -->

### 3.2.2. Tuning the POPULAR model

The POPULAR method only has one parameter : *normalize*, which can be set on *center* or *Z-score*. The *center* normalization uses the mean of the ratings, whereas the *Z-score* normalization goes further by dividing by the standard deviation, which allows this normalization method to better handle outliers.

```{r echo=FALSE}
# plot(plot_popular_normalize)
```

Our scatterplot shows...

### 3.2.3. Tuning the SVD model

### 3.2.3. Tuning the LIBMF model

## 3.3. Evaluation against the *validation* dataset

Section that presents the modeling results and discusses the model performance.

# 4. Conclusion

Section that gives a brief summary of the report, its limitations and future work

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

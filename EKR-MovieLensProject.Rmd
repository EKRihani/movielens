---
title: "MovieLens Dataset Analysis"
author: "E.K. RIHANI"
date: "15/11/2021"
output: pdf_document
indent: true   # Indent paragraphs
header-includes:
- \usepackage{indentfirst} # Indent first line of each paragraph
---

```{r setup, include=FALSE}
load("EKR-MovieLens.RData")
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(rmarkdown)
```

# 1. Overview

## 1.1 Goal of the study

The aim of this study is to analyze a MovieLens dataset and try to create a model that can predict users' future preferences based on their current preferences, using data analysis and machine learning techniques.

## 1.2 The dataset

The MovieLens datasets were created by the GroupLens research lab, from the Department of Computer Science and Engineering at the University of Minnesota.

The dataset used for this study is the 10M dataset (<https://grouplens.org/datasets/movielens/10m/>). The 10M MovieLens dataset contains `r total_number_ratings` ratings for `r total_number_movies` movies, from `r total_number_users` users.

The provided information includes the user unique ID (`r column_names[1]`), the movie unique ID (`r column_names[2]`), the rating given by the user (`r column_names[3]`), the timestamp of the rating (`r column_names[4]`), the title of the movie (`r column_names[5]`) and its genre (`r column_names[6]`).

This dataset was split into a training set (*edx*, 90% of the ratings) that will be used for building and training the recommendation system and a validation set (*validation*, 10% of the ratings) that will exclusively be used for the final evaluation of the accuracy of our recommendation system.

# 2. Methods and Analysis

This project will be articulated around the use of the *recommenderlab* package, which is a very powerful library for building recommendation systems that was recommended by Pr. Rafael Irizarry in his data science course.

We'll first briefly introduce the recommenderlab library and its different recommendation systems, then we'll evaluate their respective performances, both in computing time and accuracy, and we'll then optimize the tuning parameters of our best models before using the best model against our validation set.

## 2.1 Preparing data for the recommenderlab package

The recommenderlab package uses several specific data formats, all based on a single sparse (?) matrix, where the ratings (binary or numeric) are located at the intersection of the user row (*userId*) row (?) and the rated object (*movieId*) column. Since our rating are numeric, this study will use the *realRatingMatrix* format.

Since recommenderlab only uses ratings, one limitation of this study is that it will not use some potentially useful informations such as the moment of the rating or the year or gender of the movie to improve the accuracy of the analysis.

The first step of the data preparation involves the transformation of our dataset into a matrix

## 2.2 Comparing the performance of the different models

Several metrics can be used to evaluate the accuracy of the prediction, such as the Mean Average Error (MAE, consistent with the data units), the Mean Square Error (MSE, penalizes large errors) and the Root Mean Square Error (RMSE, penalizes large errors **and** is consistent with the data units). The metric used in this study to evaluate the accuracy of the prediction is the root-mean-squared error (RMSE), given by : $$\sqrt{ \frac{1}{N} \sum_{m,u}\left(\hat{y}_{m,u} -  y_{m,u}\right)^{2}}$$ With N the number of ratings, $\hat{y}_{m,u}$ the rating (for the movie m, by the user u) that is predicted by the model built with the training set and $y_{m,u}$ the actual rating of the testing set. The lower the RMSE, the more accurate the model.

Recommenderlab conveniently provides a *calcPredictionAccuracy* function that computes the MAE, MSE, RMSE.

Section that explains the process and techniques used, including data cleaning, data exploration and visualization, insights gained, and your modeling approach

# 3. Results

## 3.1. Performance of the different models

We can create a scatterplot of the RMSE vs compute time of our different models.

```{r echo=FALSE}
plot(plot_time_rmse1 + theme_bw())
```

As one can expect, the random model is among the fastest, but performs quite poorly in termes of accuracy.

The Popular, SVD and LIBMF are clear winners, being all in the "fast and accurate" quadrant of this plot. UBCF appears to be a reasonnable option, too. SVDF's accuracy is on par with SVD, but SVDF seems to be much slower.

The ALS seems to be quite slow, but accurate. More suprisingly, the ALS_implicit model performs very poorly (being both the slowest and most inaccurate model), despite being based on an alternating least squares strategy like the regular ALS model.

```{r, fig.show="hold", out.width="50%", echo=FALSE}
par(mar = c(4, 4, .1, .1))
  plot(plot_time_rmse2 + theme_bw())
  plot(plot_time_rmse3 + theme_bw())
```

Bigger datasets seem to confirm the respective places of the SVD, UBCF, LIBMF and Popular methods.

```{r, fig.show="hold", out.width="50%", echo=FALSE}
par(mar = c(4, 4, .2, .1))
  plot(plot_time_size1 + theme_bw())
  plot(plot_time_size2 + theme_bw())

```

The computing time vs dataset size scatterplot seems to show a lower computing time for SVD, Popular and LIBMF models, while the UBCF model appears to be considerably slower. Furthermore, the computing time of this model seems to exhibit a quadratic behaviour, whereas the others seem to be linear.

The UBCF model, while quite accurate, will probably be too slow for this study.

```{r echo=FALSE}
plot(plot_time_size3 + theme_bw())
```
We will thus select the LIBMF, POPULAR and SVD models and will tune them to improve their performance.

## 3.2. Tuning the models

### 3.2.1. Selecting the best dataset size

In order to tune our models, we first have to chose a relevant-sized training set.

```{r echo=FALSE}
plot(plot_rmse_size + theme_bw())
```

According to the RMSE vs size scatterplot, our 3 selected models seem to show stability for $size \ge 0.20$.\
Two models already have adequate performance out of the box, with $RMSE_{pop} =$ `r benchmark_result9["POPULAR",]$RMSE` in $t_{pop} =$ `r benchmark_result9["POPULAR",]$time` s for the Popular model and $RMSE_{lib} =$ `r benchmark_result9["LIBMF",]$RMSE` in $t_{lib} =$ `r benchmark_result9["LIBMF",]$time` s for LIBMF. Before tuning, SVD seems to lag behind, both in RMSE (`r benchmark_result9["SVD",]$RMSE`) and time (`r benchmark_result9["SVD",]$time` s) performance.


### 3.2.2. Tuning the POPULAR model

The POPULAR method only has one parameter : *normalize*, which defines the normalization method and can be set on *center* or *Z-score*. The *center* normalization uses the mean of the ratings, whereas the *Z-score* normalization goes further by dividing by the standard deviation, which allows this normalization method to better handle outliers.
```{r echo=FALSE}
kable(table_pop_normalize, caption = "Popular model, *normalize* parameter tuning")
```  
Quite surprisingly, our results show that the Z-score normalization, albeit more refined, produces slightly less accurate predictions than the center normalization, but is sensibly faster.

### 3.2.3. Tuning the SVD model

The SVD method has three parameters :  
* *k* : the rank of the SVD approximation (default : 10)  
* *maxiter* : the maximum number of iterations (default : 100)  
* *normalize* : the normalization method, *center* or *Z-score* (default : center)  
```{r echo=FALSE}
plot(plot_fitting5 + 
  geom_hline(yintercept = 0.9, linetype = "dotted", color = "darkred", alpha = 0.5) +   # Minimal objective
  geom_hline(yintercept = 0.865, linetype = "dotted", color = "darkgreen", alpha = 0.5) +   # Optimal objective
  theme_bw())
```  
Raising the *k* parameter vastly improves sensibly the accuracy of the prediction. However, this has a considerable cost in computing time. Getting RMSE below the optimal value (RMSE $\le$ 0.865) requires to set $k \ge$ 50 (RMSE = `r results_fitting[which(results_fitting$rmse <= 0.865 & results_fitting$model == "SVD" & results_fitting$parameter == "k")[1],]$rmse` in `r results_fitting[which(results_fitting$rmse <= 0.865 & results_fitting$model == "SVD" & results_fitting$parameter == "k")[1],]$time` s).  
```{r echo=FALSE}
plot(plot_fitting6 + theme_bw())
```  
The *maxiter* doesn't seem to have any meaningful impact on this study : all RMSE are identical, and computing time seem randomly distributed in a 2 seconds time span.  

```{r echo=FALSE}
kable(table_svd_normalize, caption = "SVD model, *normalize* parameter tuning")
```  
The *normalize* parameter does not seem to have a sensible impact on RMSE or computing time.  

In conclusion, in this study, the SVD model can be used to reach RMSE $\le$ 0.865. Better accuracy (lower RMSE) can be obtained with higher ranks of SVD approximations ($k \ge$ 50), but this accuracy has a strong cost in computing time. The *maxiter* and *normalize* factors don't seem to have a meaningful impact on accuracy or computing time.  

### 3.2.3. Tuning the LIBMF model
The LIBMF method has four parameters :  
* *dim* :  (default : 10)  
* *costp_l2* :  (default : 0.01)  
* *costq_l2* :  (default : 0.01)  
* *nthread* :  (default : 1)  
```{r echo=FALSE}
plot(plot_fitting1 + theme_bw())
```  

```{r echo=FALSE}
plot(plot_fitting2 + theme_bw())
```  

```{r echo=FALSE}
plot(plot_fitting3 + theme_bw())
```  

```{r echo=FALSE}
plot(plot_fitting4 + theme_bw())
```  

In conclusion, LIBMF seems, by far, the most interesting model, with both excellent accuracy and computing times, even with default parameters.  
The most impactful way to improve accuracy seems to increase the dimension factor (*dim*). This factor has a strong impact on computing time, but the excellent performance of the LIBMF model allows the user to choose between good accuracy and excellent computing time, or outstanding accuracy in a reasonable time.  
The user factor (*costp_l2*) and item factor (*costq_l2*) regularization parameters seem to have some minor impact, with lower values giving slightly better accuracy, with little effect in computing time.  
The number of threads has some impact in computing time. However, this impact looks quite minor. This can probably be explained by the small size of the sample used for fitting purposes and resulting short computation time : the gains obtained by multithreading thus cannot overcome the time and memory used to setup the parallelization (parallelism overhead).

## 3.3. Evaluation against the *validation* dataset

*Section that presents the modeling results and discusses the model performance.*

# 4. Conclusion

*Section that gives a brief summary of the report, its limitations and future work*
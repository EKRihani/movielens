---
title: "MovieLens Dataset Analysis"
author: "E.K. RIHANI"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    number_sections: true   # Add section numbering
    toc: true
    toc_depth: 3

header-includes:
indent: true   # Indent paragraphs
bibliography: packages.bib #, references.bib
references:
  - id: test2000
    title: Le test du test dans le test
    author: 
    - family: Testeur
      given: Johnny
    url: http://dx.doi.org/10.1038/nmat3283
    DOI: 10.1038/nmat3283
    publisher: Journal of Tested tests
    type: article-journal
    issued:
      year: 2018
csl: https://www.zotero.org/styles/vancouver-superscript
#- \usepackage{indentfirst} # Indent first line of each paragraph
---

```{r setup, include=FALSE}
load("EKR-MovieLens.RData")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(ggpubr)) install.packages("ggpubr", repos = "http://cran.us.r-project.org")
if(!require(rmarkdown)) install.packages("rmarkdown", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
library(ggplot2)
library(ggpubr)   # Combine plots (ggarrange)
library(rmarkdown)
library(knitr)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
knitr::write_bib(c("tidyverse", "caret", "data.table", "reshape2", "ggplot2", "ggrepel", "recommenderlab", "stringr", "ggpubr", "rmarkdown", "knitr"), "packages.bib")
```
\newpage
\section{Overview}
\subsection{Goal of the study}
\paragraph*{}
The aim of this study[@test2000] is to analyze a MovieLens dataset and try to create a model that can predict users' future preferences based on their current preferences, using data analysis and machine learning techniques.

\subsection{The dataset}
\paragraph*{}
The MovieLens datasets were created by the GroupLens research lab, from the Department of Computer Science and Engineering at the University of Minnesota.
\paragraph*{}
The dataset used for this study is the 10M dataset (<https://grouplens.org/datasets/movielens/10m/>). The 10M MovieLens dataset contains `r total_number_ratings` ratings for `r total_number_movies` movies, from `r total_number_users` users, which translates into a `r total_number_movies` $\times$ `r total_number_users` matrix, with `r round(total_number_movies*total_number_users/1e6, 1)` millions cells, `r round((1 - total_number_ratings/(total_number_movies*total_number_users))*100, 1)` % of which are empty. 
\paragraph*{}
In layman's terms, the goal of this project is to fill these empty cells as accurately as possible.
\paragraph*{}
The provided information includes the user's unique ID (`r column_names[1]`), the movie's unique ID (`r column_names[2]`), the rating given by the user (`r column_names[3]`), the timestamp of the rating (`r column_names[4]`), the title of the movie (`r column_names[5]`) and its genre (`r column_names[6]`).
\paragraph*{}
This dataset was split into a training set (*edx*, 90% of the ratings) that will be used for building, training and tuning the recommendation system and a validation set (*validation*, 10% of the ratings) that will exclusively be used for the final evaluation of the accuracy of our recommendation system.

\newpage
\section{Methods and Analysis}
\paragraph*{}
This project will be articulated around the use of the *recommenderlab* package, which is a very powerful library for building recommendation systems that was recommended by Pr. Rafael Irizarry in his data science course.
\paragraph*{}
The configuration will be described, since it can have an impact on results and performance.Then recommenderlab library and its different recommendation systems will be briefly introduced. Their respective performances will be evaluated, both in computing time and accuracy, in order to select the most promising models. Their tuning parameters will then be tuned, and the best model will finally be used against the validation set.
\paragraph*{}
Code extracts will be included to illustrate how to setup and use the *recommenderlab* package.

\subsection{Computer and R configuration}
\paragraph*{}
The code, benchmarks and report were run on the following computer configuration :

* CPU : AMD Ryzen 5 5600G
* RAM : 2x16 GB DDR4-3200
* SSD : Crucial P5 M2 NVMe
* OS : Xubuntu Linux
* R : version 4.1.0
* Packages : tidyverse[@R-tidyverse] (v1.3.1), caret[@R-caret] (v6.0-88), data.table[@R-data.table] (v1.14.0), reshape2[@R-reshape2] (v1.4.4), ggplot2[@R-ggplot2] (v3.3.5), ggrepel[@R-ggrepel] (v0.9.1), recommenderlab[@R-recommenderlab] (v0.2-7), stringr[@R-stringr] (v1.4.0), **[parallel????]**, ggpubr[@R-ggpubr] (v0.4.0), rmarkdown[@R-rmarkdown] (v2.11), knitr[@R-knitr] (v1.34).

\paragraph*{}
The code was also tested on a Windows 7 x64 SP1 computer with 16 GB RAM. It was not tested on any Mac OS system.
\subsection{Preparing data for the recommenderlab package}
\paragraph*{}
The recommenderlab package uses several specific data formats, all based on a single sparse matrix, where the ratings (binary or numeric) are located at the intersection of the user (*userId*) row and the rated object (*movieId*) column. Because ratings are numeric, this study will use the *realRatingMatrix* format.
\paragraph*{}
Since recommenderlab only uses ratings, one notable limitation of this study is that it will not use some potentially useful information such as the moment of the rating or the year or gender of the movie to improve the accuracy of the analysis.
\paragraph*{}
The first step of the data preparation involves the transformation of the *edx* dataset into a $userId \times movieId$ matrix (*edx_rrm*) with *rating* at each user/movie intersection. The acast::reshape2 function can be used to perform this operation. The resulting matrix can then be easily converted in a *realRatingMatrix* format.

```{r, eval = FALSE}
edx_rrm <- acast(edx, userId ~ movieId, value.var = "rating")
edx_rrm <- as(edx_rrm, "realRatingMatrix")
```

The same operation must be performed on the validation data set. To make sure the *validation* isn't used until the final validation, we explicitly chose not to use it *at all* until the final steps.
\paragraph*{}
However, some recommenderlab models may encounter problems if the training set and the validation sets don't have the same items (movies). This can be the case if a list-splitting method is used, because some items can be present in the training set, but not in the validation set.
\paragraph*{}
Two equivalent strategies can be used : either add the missing items with *NA* ratings in the validation set or remove the useless items in the training set. Since modifications of the validation set are explicitly forbidden, the only option is to remove useless items in the training set, i.e. exclude from the *edx* set all movies that don't exist in the *validation* set, before building the realRatingMatrix.
\paragraph*{}
Preliminary benchmarks can be run by splitting the training realRatingMatrix (*edx_rrm*) into smaller subsets, or in training/prevalidation datasets by using a regular index sampling method. For example, for a 90/10 training/prevalidation ratio :

```{r, eval = FALSE}
test_index <- sample(x = seq(1, nrow(edx_rrm)), size = nrow(edx_rrm)*0.1, replace = FALSE)
edx_rrm_train <- edx_rrm[-test_index]
edx_rrm_test <- edx_rrm[test_index]
```

While splitting matrices for training/pre-validation purposes makes the code much cleaner and faster, it is *not* strictly equivalent to splitting a list and then building the corresponding matrices. On some very specific datasets, the results obtained in the pre-validation and tuning stages may vary when one uses a matrix-based splitting or a list-based splitting.
\paragraph*{}
However, the choice of the list vs matrix splitting strategy during the intermediate benchmarking/tuning steps has no impact on the final RMSE, since the *edx* and *validation* datasets were already created using a list-based splitting method with a fixed seed.

\subsection{Performance metrics}
\paragraph*{}
Several metrics can be used to evaluate the accuracy of the prediction, such as the Mean Average Error (MAE, consistent with the data units), the Mean Square Error (MSE, penalizes large errors) and the Root Mean Square Error (RMSE, penalizes large errors **and** is consistent with the data units). The metric used in this study to evaluate the accuracy of the prediction is the root-mean-squared error (RMSE), given by : $$\sqrt{ \frac{1}{N} \sum_{m,u}\left(\hat{y}_{m,u} -  y_{m,u}\right)^{2}}$$ With N the number of ratings, $\hat{y}_{m,u}$ the rating (for the movie m, by the user u) that is predicted by the model built with the training set and $y_{m,u}$ the actual rating of the testing set. The lower the RMSE, the more accurate the model.
\paragraph*{}
Recommenderlab conveniently provides a *calcPredictionAccuracy* function that computes the MAE, MSE, RMSE. As an example (using a LIBMF method) the recommendation, prediction and accuracy evaluation can be run by :

```{r, eval = FALSE}
recommendation <- Recommender(data = edx_rrm, method = "LIBMF")
prediction <- predict(recommendation, validation_rrm, type = "ratingMatrix")
accuracy <- calcPredictionAccuracy(validation_rrm, prediction)
```

Another very useful and easy to evaluate metric is the time needed to execute the recommendation, prediction and accuracy steps. The code used in this study is quite simple :
```{r, eval = FALSE}
start_time <- Sys.time()
# Insert_code_here #
end_time <- Sys.time()
running_time <- difftime(end_time, start_time, units = "secs")
```
The *units* argument in the *difftime* function ensures all units are kept consistent. Mixing minutes and seconds can be problematic, especially if these results are used for plotting.

\subsection{About the recommenderlab methods}
\paragraph*{}
The *Random* method, as its name suggests, assigns random ratings to all movies for each user. This method can be used to benchmark other models.
\paragraph*{}
The *Popular* method is based on the most rated items, and will thus recommend the most viewed movies. The rating is evaluated using a distance method.
\paragraph*{}
The *UBCF* and *IBCF* methods are respectively an user-based and an item-based collaborating filtering methods. These methods are the oldest recommender methods (1992) and are based on a "similar users like similar things" strategies. UBCF focuses on the users and is based on the hypothesis that users with comparable preferences will rate movies similarly, and thus tries to find for each user a k-neighborhood of the most comparable users (by using cosine similarity, by default) and aggregate their ratings to form a prediction. IBCF works similarly, but is focused on items, the core hypothesis being that users prefer movies that are similar to movies they already like. The IBCF method uses a comparable approach as the UBCF but tries to find similarities between movies by computing the k-neighborhood of movies instead of users.
\paragraph*{}
The *ALS*, *ALS_implicit* and *LIBMF* methods are all based on the same mathematical concept : matrix factorization. Matrix factorization is a widely used method that tries to approximate our entire rating matrix $R_{u \times m}$ as the product of smaller-sized matrices $P_{k \times u}$ and $Q_{k \times m}$ : $R \approx P'Q$. In other words, each user and each item are summarized by *k* dimensional vectors, where *k* is a small fixed number. The underlying mathematical problem is then to minimize the distance (least squares error) between each known $r_{u,m}$ rating and the corresponding intersection of the product of our two smaller-sized matrices $p_{u}^{T}.q_{m}$.
\paragraph*{}
In ALS and ALS_implicit optimization strategies, $p_{u}$ is fixed and $q_{m}$ is optimized by minimizing the square error, then $q_{m}$ is fixed and then $p_{u}$ is optimized, hence the *Alternating Least Square* name. The ALS method is used for explicit data, which are strictly correlated to the values we are trying to predict. The ALS_implicit method is designed for implicit data that reflects the interaction between the user and the movie and is only indirectly linked to the rating. In our application, the ALS method will try to predict user ratings using user ratings, whereas the ALS_implicit method tries to predict user ratings with clicks, fast-forwards, number of times viewed... Since these kind of data isn't present in our dataset, using the ALS_implicit method makes little sense.
\paragraph*{}
*LIMBF* is an open-source library was created by Chin et al, from the Taiwan University, in 2014. This algorithm uses a gradient descent approach, which is an iterative algorithm that compute the local gradient of this distance and adjusts the $p_{u}$ and $q_{m}$ *against* the gradient in order to approach a better value. This specific library also aims to make full use of the computing power of modern processors (SSE and AMX instructions, multithreading) in Matrix Factorization.
\paragraph*{}
The *SVD* and *SVDF* (Funk SVD) methods are both Singular Value Decomposition methods **[ A FINIR !!!!]**

\subsection{Performance of the different models}
\paragraph*{}
Some models may be faster and/or more accurate than others and thus be more interesting for our study. In order to evaluate the model performance, we will use smaller subsets that will allow quicker computation and comparison of the respective performance of all the recommenderlab methods.
\paragraph*{}
The *edx* dataset was split into smaller subsets that will allow a quicker evaluation of our different models. These smaller subsets are then split into a training (90%) and validation (10%) sets.
\paragraph*{}
Training and validation are then performed for each dataset. The computing time of the training, prediction and validation steps is measured. The accuracy of the prediction (RMSE) is also evaluated. We can then create a scatterplot of the RMSE vs compute time of our different models, and plot lines for our RMSE criterias (RMSE > 0.900 and RMSE < 0.865).
\paragraph*{}
```{r echo = FALSE, fig.cap = "Benchmark of the recommenderlab methods with a 0.5% subset size"}
plot(plot_time_rmse1 + theme_bw())
```
```{r echo = FALSE}
kable(benchmark_result1 %>% select("RMSE", "time"),
  align = "cc",
  caption = "Benchmark of the methods on a 0.5% subset")
```

As one can expect, the random model is among the fastest, but performs quite poorly in terms of accuracy.
\paragraph*{}
The Popular, SVD, UBCF and LIBMF are clear winners, being all in the "fast and accurate" quadrant of our RMSE vs time plot. The accuracy of the SVDF method is comparable with the accuracy of SVD, but SVDF seems to be much slower.
\paragraph*{}
The ALS method seems to be quite slow, but accurate. As expected, we can see a large difference between explicit and implicit ALS methods. Our data are explicit : we use (known) user ratings to predict (unknown) user ratings. Force-feeding these explicit data in an method that was designed around implicit data gives results that are both very slow and inaccurate ($RMSE_{ALSimplicit} > RMSE_{random}$). 
\paragraph*{}
As said before, the UBCF method performed quite well. However, the IBCF was so slow it wasn't included in the plot. This can look quite suprising at first. However, the UBCF is based on users, while the IBCF is based on items (movies). In other words, UBCF is row-based, while IBCF is column-based : since we defined our smaller datasets as a smaller matrix row-wise, the UBCF worked on 0.5% of the rows, while the IBCF method had to work on 100% of the columns, hence the poor performance, both on time and accuracy criterias.
\paragraph*{}
The ALS_implicit and IBCF results can look quite surprising at first, but were predictable in the end. This underline the importance of understanding how these methods work, and not just treat them as "black boxes" or "magic bullets".
\paragraph*{}
We can use these preliminary results to exclude the slowest and most inaccurate models (SVDF, ALS, ALS_implicit, random) for our next benchmarks with larger datasets.
\paragraph*{}
```{r, echo = FALSE, fig.cap = "Benchmark of the recommenderlab models with 2% and 10% subset sizes"}
plot(
  ggarrange(
  plot_time_rmse2 + theme_bw(),
  plot_time_rmse3 + ylab("")+ theme_bw()
  )
)
```

These larger datasets seem to confirm the respective places of the SVD, UBCF, LIBMF and Popular methods : SVD and Popular are fast but not very accurate, UBCF is quite accurate but slow. LIBMF seems to be the most interesting model, being both very accurate and fast.
\paragraph*{}
However, one should keep in mind that these results -- both in time and accuracy -- are for very small datasets. We should thus carry on and try to predict the performance of these models on larger datasets.
\paragraph*{}
```{r, echo = FALSE, fig.cap = "Computing time of the 4 best models (linear and quadratic time scales)"}
plot(
  ggarrange(
  plot_time_size1 + theme_bw(),
  plot_time_size2 + ylab("") + theme_bw(),
  common.legend = TRUE, legend = "right"
  )
)
```
\paragraph*{}
The computing time vs dataset size scatterplot seems to show a lower computing time for SVD, Popular and LIBMF models, while the UBCF model appears to be considerably slower. Furthermore, the computing time of this model seems to exhibit a quadratic behavior (right plot), whereas the other methods seem to be linear.
\paragraph*{}
We can try to predict the computing time for a full-sized dataset :
```{r echo = FALSE}
kable(tsv_models,
  align = "cccccc",
  caption = "Modelling of time vs size behavior of the 4 best models",
  digits = c(0,0,3,2,1,3))
```

\paragraph*{}
The UBCF model, while quite accurate, will probably be too slow for this study.
\paragraph*{}
We will thus select the LIBMF, POPULAR and SVD models. Since all the previous results were obtained with default settings, the next step will be to tune them in order to improve their performance.

\subsection{Tuning the models}

\subsubsection{Selecting the best dataset size}
\paragraph*{}
In order to tune our models, we first have to chose a relevant-sized training set.
\paragraph*{}
```{r out.width = "90%", echo = FALSE}
plot(plot_rmse_size + theme_bw())
```
\paragraph*{}
According to the RMSE vs size scatterplot, our 3 selected models seem to show stability for $size \ge 0.20$.\
Two models already have adequate performance out of the box, with $RMSE_{pop} =$ `r benchmark_result9["POPULAR",]$RMSE` in $t_{pop} =$ `r benchmark_result9["POPULAR",]$time` s for the Popular model and $RMSE_{lib} =$ `r benchmark_result9["LIBMF",]$RMSE` in $t_{lib} =$ `r benchmark_result9["LIBMF",]$time` s for LIBMF. Before tuning, SVD seems to lag behind, both in RMSE (`r benchmark_result9["SVD",]$RMSE`) and time (`r benchmark_result9["SVD",]$time` s) performance.

\subsubsection{Tuning the POPULAR model}
\paragraph*{}
The POPULAR method only has one parameter : *normalize*, which defines the normalization method used to counter the user biases. This parameter can be set on *center* or *Z-score*. The *center* normalization uses the mean of the ratings, whereas the *Z-score* normalization goes further by dividing by the standard deviation, which allows this normalization method to better handle outliers.
```{r echo = FALSE}
kable(table_pop_normalize, caption = "Popular model, *normalize* parameter tuning")
```
\paragraph*{}
Quite surprisingly, our results show that the Z-score normalization, albeit more refined, produces slightly less accurate predictions than the center normalization, but is sensibly faster.  
\paragraph*{}
In conclusion, the Popular method, while quite simple to understand and to use, is not accurate enough (RMSE = `r min(table_pop_normalize$rmse)` ) for our study.

\subsubsection{Tuning the SVD model}
\paragraph*{}
The SVD method has three parameters :

* *k* : the rank of the SVD approximation (default : 10)
* *maxiter* : the maximum number of iterations (default : 100)
* *normalize* : the normalization method, *center* or *Z-score* (default : center
\paragraph*{}
```{r out.width = "90%", echo = FALSE}
plot(plot_fitting5 + 
  geom_hline(yintercept = 0.9, linetype = "dotted", color = "darkred", alpha = 0.5) +   # Minimal objective
  geom_hline(yintercept = 0.865, linetype = "dotted", color = "darkgreen", alpha = 0.5) +   # Optimal objective
  theme_bw())
```
\paragraph*{}
Raising the *k* parameter vastly improves the accuracy of the prediction. However, this has a considerable cost in computing time. Getting RMSE below the optimal value (RMSE $\le$ 0.865) requires to set $k >$ 50 (RMSE = `r results_fitting[which(results_fitting$rmse <= 0.865 & results_fitting$model == "SVD" & results_fitting$parameter == "k")[1],]$rmse` in `r results_fitting[which(results_fitting$rmse <= 0.865 & results_fitting$model == "SVD" & results_fitting$parameter == "k")[1],]$time` s for *k* = `r results_fitting[which(results_fitting$rmse <= 0.865 & results_fitting$model == "SVD" & results_fitting$parameter == "k")[1],]$value`).
\paragraph*{}
```{r out.width = "90%", echo = FALSE}
plot(plot_fitting6 + theme_bw())
```
\paragraph*{}
The *maxiter* doesn't seem to have any meaningful impact on this study : all RMSE are identical, and all computing times seem randomly distributed in a `r round(max(results_fitting[which(results_fitting$model == "SVD" & results_fitting$parameter == "maxiter"),]$time) - min(results_fitting[which(results_fitting$model == "SVD" & results_fitting$parameter == "maxiter"),]$time),1)` seconds time span.
\paragraph*{}
```{r out.width = "90%", echo = FALSE}
kable(table_svd_normalize, caption = "SVD model, *normalize* parameter tuning")
```
\paragraph*{}
The *normalize* parameter does not seem to have a sensible impact on RMSE or computing time.
\paragraph*{}
In conclusion, the SVD model can be used to reach RMSE $\le$ 0.865. Better accuracy (lower RMSE) can be obtained with higher ranks of SVD approximations ($k >$ 50), but this accuracy has a strong cost in computing time. The *maxiter* and *normalize* factors don't seem to have a meaningful impact on accuracy or computing time.  

\subsubsection{Tuning the LIBMF model}
\paragraph*{}
The LIBMF method has four parameters :

* *dim* : the number of latent features (default : 10)
* *costp_l2* : the regularization parameter for the user factor (default : 0.01)
* *costq_l2* : the regularization parameter for the item factor (default : 0.01)
* *nthread* : the number of threads (default : 1)

\paragraph*{}
```{r out.width = "90%", echo = FALSE}
plot(plot_fitting1 + theme_bw())
```
\paragraph*{}
The *dim* parameter has a major effect on accuracy and computing time. Higher values ($dim = 1000$) yield a better accuracy, with RMSE = `r results_fitting[results_fitting$model == "LIBMF" & results_fitting$parameter == "dim" & results_fitting$value == "1000",]$rmse`, at the cost of a higher computing time (t = `r results_fitting[results_fitting$model == "LIBMF" & results_fitting$parameter == "dim" & results_fitting$value == "1000",]$time`).
\paragraph*{}
Very high or very low dim values seem to show diminishing returns in time or accuracy. We can thus design time/RMSE optimization strategies by computing a composite indicator (such as $time \times RMSE^{n}$, with $n$ a fixed weight factor), then finding the lowest value.
\paragraph*{}
We can for example choose to put a 2:1 weight on RMSE vs time (*n* = 2), to reflect the higher priority given to RMSE -- which is the primary goal of this project -- while also taking computing time into account.
```{r out.width = "90%", echo = FALSE}
plot(plot_fitting1b + theme_bw())
```
\paragraph*{}
For *n* = 2, an optimum is reached with $dim \approx 400$.
\paragraph*{}
```{r, echo = FALSE, fig.cap = "Fitting of LIBMF method, user (costp_l2) and item (costq_l2) factor regularization parameters"}
plot(ggarrange(
  plot_fitting2 + ggtitle("Fitting: LIBMF method, costp_l2") + theme_bw(),
  plot_fitting3 + ggtitle("Fitting: LIBMF method, costp_l2") + ylab("") + theme_bw()
  )
)
```

\paragraph*{}
```{r out.width = "90%", echo=FALSE}
plot(plot_fitting4 + theme_bw())
```

\paragraph*{}
In conclusion, LIBMF seems to be, by far, the most interesting model, with both excellent accuracy and computing times, even with default parameters.
\paragraph*{}
The most impactful way to improve accuracy seems to increase the number of latent features (*dim*). However, this factor has a strong impact on computing time. Our results are perfectly consistent with those obtained by the creators of the LIBMF library in their initial tests. The excellent performance of the LIBMF model allows the user to choose between good accuracy and excellent computing time, or outstanding accuracy in a reasonable time
\paragraph*{}
The user factor (*costp_l2*) and item factor (*costq_l2*) regularization parameters seem to have some minor impact, with lower values giving slightly better accuracy, with little effect in computing time. However, since the LIMBF model uses these factors to penalize potential overfitting, it may be preferable to keep the default values.
\paragraph*{}
The number of threads has some impact in computing time. However, this impact looks quite minor. This can probably be explained by the small size of the sample used for our fitting purposes and the resulting short computation time : the gains obtained by multithreading thus cannot overcome the time and memory used to setup the parallelization (parallelism overhead).

\paragraph*{}
*Section that explains the process and techniques used, including data cleaning, data exploration and visualization, insights gained, and your modeling approach*

\newpage
\section{Results}

\subsection{Evaluation against the \emph{validation} dataset}
\paragraph*{}
Given the previous benchmarks, we chose the following parameters for our final evaluation, against the *validation* dataset :

* Method : LIMBF
* Number of latent features (*dim*) : 400
* Regularization parameter, user factor (*costp_l2*) : 0.01
* Regularization parameter, user factor (*costq_l2*) : 0.01
* Number of threads (*nthread*) : 16


We manage to obtain RMSE = `r final_rmse` in `r final_time` s.
*Section that presents the modeling results and discusses the model performance.*


\newpage
\section{Conclusion}

*Section that gives a brief summary of the report, its limitations and future work*

\newpage
\section{References}